tokenizer:
  model_name: gpt2
  padding_side: left
  truncation_side: right
  pad_token_as_eos_token: True
  max_length: 128

datapool:
  id: "daily_dialog"
  num_train_samples: 5000
  train_samples_rnd_seed: 7
  args:
    context_size: 5

alg:
  model_type: causal
  model_name: gpt2
  generation_kwargs:
    do_sample: True
    top_k: 20
    min_length: 2
    max_new_tokens: 20
    post_processing_fn: null
#    num_beams: 5
#    min_length: 5
#    max_new_tokens: 20
#    post_processing_fn: null

evaluation:
  eval_batch_size: 256
  num_samples_per_input: 10
  metrics:
    - id: meteor
      args: { }

