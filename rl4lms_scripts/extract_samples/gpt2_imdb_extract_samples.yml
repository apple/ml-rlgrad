tokenizer:
  model_name: lvwerra/gpt2-imdb
  padding_side: left
  truncation_side: left
  pad_token_as_eos_token: True
  max_length: 64


datapool:
  id: imdb
  num_train_samples: 5000
  train_samples_rnd_seed: 7
  args:
    seed: 42
    only_positive_samples: True

alg:
  model_type: causal
  model_name: lvwerra/gpt2-imdb
  generation_kwargs:
    do_sample: True
    min_length: 48
    max_new_tokens: 48
    post_processing_fn: null
#    num_beams: 5
#    min_length: 5
#    max_new_tokens: 20
#    post_processing_fn: null

evaluation:
  eval_batch_size: 128
  num_samples_per_input: 10
  metrics:
    - id: learned_reward
      args:
        model_name: lvwerra/distilbert-imdb
        label_ix: 1
        batch_size: 100

